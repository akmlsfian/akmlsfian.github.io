[
  {
    "id": 1,
    "title": "AskCPG",
    "category": "RAG & Generative AI",
    "role": "Frontend Architect",
    "image": "/images/portfolio/AskCPG/page.png",
    "icon": "/images/portfolio/AskCPG/icon.png",
    "problem": "Clinical Practice Guidelines (CPGs) are dense, static PDF documents that are difficult for doctors to query in real-time during patient consultations. The system needed to deliver instant, hallucination-free answers from thousands of pages with source citations.",
    "solution": "Architected a real-time RAG (Retrieval-Augmented Generation) interface using a custom Server-Sent Events (SSE) streaming engine. Implemented a dual-stream architecture that renders citations parallel to generative text, ensuring clinical trust.",
    "architecture": "The frontend consumes a custom SSE stream (`text/event-stream`) via a specialized `StreamingService` class. Unlike standard `EventSource`, this implementation manually parses raw chunks to handle custom events (`onInterface`, `onSystem`) alongside content delta updates, allowing for rich UI state changes driven by the backend stream.",
    "performance": "Achieved <200ms Time-to-First-Token (TTFT) by bypassing standard JSON request/response cycles. Implemented an optimistic UI that renders 'thinking' states based on stream control signals before content arrives.",
    "constraints": [
      "Strict data residency requirements (healthcare data)",
      "Must function on low-bandwidth hospital networks",
      "Zero-tolerance for hallucinated medical advice"
    ],
    "decisions": [
      "Chose SSE over WebSockets for simpler unidirectional firewall traversal in hospital networks",
      "Implemented 'Manual Chunk Parsing' instead of 'fetch-event-source' library to handle custom non-standard control events within the stream",
      "Decoupled stream handling from UI components using a Service-based pattern"
    ],
    "tradeoffs": [
      "Manual stream parsing increased codebase complexity but provided necessary granular control over UI states",
      "No bidirectional communication in the stream channel (requires separate POST requests for user input)"
    ],
    "impact": [
      "Reduced information retrieval time from ~3 minutes to ~5 seconds",
      "Deployed to 15+ government hospitals",
      "Processed 50,000+ clinical queries in pilot phase"
    ],
    "stack": ["Vue 3", "Server-Sent Events", "Firebase Auth", "Pinia", "Tailwind"],
    "links": {
      "demo": "https://askcpg.qmed.asia",
      "repo": null
    },
    "gallery": [
      "/images/portfolio/AskCPG/page.png",
      "/images/portfolio/AskCPG/Screenshot 2026-02-11 175923.png"
    ]
  },
  {
    "id": 2,
    "title": "DrMata AI",
    "category": "Medical AI & IoT",
    "role": "Full Stack Engineer",
    "image": "/images/portfolio/Drmata/drmata session.png",
    "icon": "/images/portfolio/Drmata/icon.png",
    "problem": "Diabetic Retinopathy (DR) screening requires expensive fundus cameras and specialists. Integrating diverse legacy hardware (Topcon, Zeiss, Kowa) with cloud-based AI analysis posed significant interoperability and bandwidth challenges.",
    "solution": "Built a hardware-agnostic 'Device Adapter' pattern in the frontend that normalizes image data from various camera SDKs before upload. Implemented a resilient upload queue for high-resolution retinal images (10MB+) that handles intermittent clinic internet connectivity.",
    "architecture": "The system uses a Service Layer pattern (`imageProcessServices.js`) to decouple UI from complex API logic. It features a custom `HttpChunker` to handle large payload responses and integrates directly with Python backends for RWW (Retinal Vessel Width) segmentation.",
    "performance": "Optimized large image rendering using HTML5 Canvas with downsampling for preview, while streaming full-resolution data to the backend. Implemented aggressive caching for static assets in rural deployments.",
    "constraints": [
      "Interoperability with proprietary camera SDKs (Topcon, Zeiss, Kowa)",
      "Strict 30-second SLA for AI analysis results",
      "Offline-capable for rural deployments"
    ],
    "decisions": [
      "Used a 'Store-and-Forward' architecture for images: capture locally, queue for upload, process async",
      "Implemented client-side image validation to reject poor quality scans before wasting bandwidth",
      "Separated 'Admin' and 'User' apps to reduce bundle size and enforce security boundaries"
    ],
    "tradeoffs": [
      "Client-side validation adds CPU load to clinic PCs but saves significant bandwidth",
      "Maintaining two separate frontend codebases increases maintenance but ensures stricter security compliance"
    ],
    "impact": [
      "Enabled screening of 10,000+ patients in rural areas",
      "Compatible with 90% of market-leading fundus cameras",
      "Reduced false rejection rate by 40% via client-side pre-validation"
    ],
    "stack": ["Vue 3", "Python Integration", "Canvas API", "Firebase", "Axios"],
    "links": {
      "demo": "https://www.drmata.com.my",
      "repo": null
    },
    "gallery": [
      "/images/portfolio/Drmata/drmata session.png",
      "/images/portfolio/Drmata/retina viewer.png"
    ]
  },
  {
    "id": 3,
    "title": "Nora Triage",
    "category": "Mobile Health",
    "role": "Mobile Architect",
    "image": "/images/portfolio/norakpj/startpage.png",
    "icon": "/images/portfolio/norakpj/icon.png",
    "problem": "Patient triage in emergency departments is high-pressure and time-sensitive. The app needed to work reliably across iOS and Android devices with varying hardware capabilities, often in dead zones within hospitals.",
    "solution": "Engineered a 'Hybrid-Native' architecture using Quasar (Vue) and Capacitor. Implemented an 'Adapter Pattern' (`src/adapters/`) to abstract native device capabilities (Clipboard, Geolocation, Filesystem), allowing the same codebase to run in Web, iOS, and Android environments seamlessly.",
    "architecture": "The app uses a 'Local-First' data strategy. Critical triage logic is bundled within the app, minimizing API dependency. Capacitor plugins bridge the gap for hardware access, while the Vue layer handles complex form logic and state management.",
    "performance": "Achieved 60fps animations on mid-range Android devices by offloading heavy computations to Web Workers. Utilized Quasar's tree-shaking to keep the initial bundle size under 300KB for rapid loading.",
    "constraints": [
      "Must support offline operation for up to 4 hours",
      "Strict data encryption at rest (device storage)",
      "Cross-platform consistency (iOS/Android/Web)"
    ],
    "decisions": [
      "Adopted the Adapter Pattern for all native APIs to allow mock implementations for browser-based testing",
      "Used Quasar Framework for its battle-tested Material Design components, reducing custom UI development time",
      "Implemented 'Optimistic Updates' for triage submission to make the UI feel instant despite network latency"
    ],
    "tradeoffs": [
      "Larger app binary size compared to pure native, but massive code reuse across platforms",
      "Dependency on Capacitor ecosystem for native features"
    ],
    "impact": [
      "Reduced triage time by 30% per patient",
      "Deployed to 5 KPJ Specialist Hospitals",
      "99.9% crash-free sessions on production"
    ],
    "stack": ["Quasar", "Capacitor", "Vue 3", "TypeScript", "iOS/Android"],
    "links": {
      "demo": null,
      "repo": null
    },
    "gallery": [
      "/images/portfolio/norakpj/startpage.png",
      "/images/portfolio/norakpj/symptom checker.png"
    ]
  },
  {
    "id": 4,
    "title": "QmedScribe",
    "category": "Ambient AI Scribe",
    "role": "Lead Frontend Engineer",
    "image": "/images/portfolio/qmedScribe/home page.png",
    "icon": "/images/portfolio/qmedScribe/icon.png",
    "problem": "Doctors spend 40% of their time typing notes. Real-time transcription requires continuous audio streaming which usually freezes the main thread, causing UI jank. Latency in VAD (Voice Activity Detection) leads to cut-off sentences.",
    "solution": "Implemented a multi-threaded audio architecture. Used `AudioWorklet` (`vad-processor.js`) to process raw PCM audio samples off the main thread. Integrated `onnxruntime-web` with WASM to run a lightweight Silero VAD model directly in the browser.",
    "architecture": "The system employs a 'Hybrid Edge-Cloud' approach. The browser handles VAD and silence truncation using WebAssembly, sending only speech segments to the cloud. The response uses a custom `HttpChunker` to stream LLM tokens in real-time.",
    "deepDive": {
      "description": "The audio pipeline runs entirely isolated from the UI thread. Raw audio -> AudioWorklet (Buffer) -> ONNX WASM (VAD Inference) -> Main Thread (WebSocket/HTTP). This ensures the UI remains buttery smooth (60fps) even while processing 16kHz audio streams and rendering real-time waveform visualizations.",
      "nodes": [
        {"id": "mic", "label": "Microphone Input (16kHz)", "type": "input"},
        {"id": "worklet", "label": "AudioWorklet (Processor)", "type": "process"},
        {"id": "wasm", "label": "ONNX Runtime (WASM)", "type": "ai"},
        {"id": "main", "label": "Main Thread (UI)", "type": "ui"},
        {"id": "cloud", "label": "Cloud LLM", "type": "cloud"}
      ]
    },
    "performance": "Reduced upload bandwidth by 60% by filtering silence at the edge. VAD latency <20ms using WASM. UI maintains 60fps during active recording.",
    "constraints": [
      "Real-time processing with <500ms latency",
      "Must work on low-end clinic laptops (4GB RAM)",
      "Strict privacy (no audio storage on client)"
    ],
    "decisions": [
      "Moved VAD to the client-side to reduce server costs and latency",
      "Used AudioWorklets instead of ScriptProcessorNode to avoid main-thread blocking",
      "Implemented a 'Ring Buffer' strategy for audio data to prevent memory leaks during long sessions"
    ],
    "tradeoffs": [
      "Initial WASM model load (2MB) adds to startup time",
      "Browser compatibility limited to modern browsers supporting AudioWorklet"
    ],
    "impact": [
      "Saved doctors ~2 hours of documentation time daily",
      "Processed 10,000+ hours of clinical consultations",
      "Zero audio loss incidents in production"
    ],
    "stack": ["Vue 3", "AudioWorklet", "ONNX WASM", "TypeScript", "Pinia"],
    "links": {
      "demo": "https://scribe.qmed.asia",
      "repo": null
    },
    "gallery": [
      "/images/portfolio/qmedScribe/home page.png",
      "/images/portfolio/qmedScribe/audio panel.png",
      "/images/portfolio/qmedScribe/result.png",
      "/images/portfolio/qmedScribe/template.png"
    ]
  }
]
