[
  {
    "id": 1,
    "title": "AskCPG",
    "category": "Medical AI",
    "role": "Lead Frontend Architect",
    "problem": "Malaysian medical professionals faced a knowledge retrieval bottleneck, requiring 10-15 minutes to locate specific protocols within 500+ page Clinical Practice Guidelines (CPG) PDFs, leading to decision fatigue.",
    "solution": "Architected a RAG (Retrieval-Augmented Generation) interface that delivers citation-backed answers in under 2 seconds. Implemented a streaming response engine to handle LLM token generation without UI blocking.",
    "architecture": "Vue 3 Composition API driving a reactive stream parser. Edge-cached vector queries via Python backend. Client-side citation mapping using virtualization for large document rendering.",
    "performance": "Reduced average query time from 12m to 1.8s. Achieved 98 Lighthouse Performance score via route-level code splitting and tree-shaking of heavy PDF libraries.",
    "constraints": ["Strict data sovereignty (servers must be local)", "Mobile-first for ward usage", "Zero-trust verification (must show exact PDF page source)"],
    "decisions": [
      "Chose Server-Sent Events (SSE) over WebSockets for unidirectional token streaming to reduce handshake overhead.",
      "Implemented 'Optimistic UI' citation placeholders to reduce perceived latency while vector search completes."
    ],
    "tradeoffs": [
      "Trade-off: Client-side PDF rendering vs Server-side rasterization. \nDecision: Client-side using PDF.js workers to reduce server egress costs, accepting slightly higher initial memory footprint."
    ],
    "impact": "Deployed to 40+ clinics. Reduced information retrieval time by ~90%. Processed 15,000+ queries in first quarter with zero downtime.",
    "stack": ["Vue.js", "Server-Sent Events", "PDF.js Workers", "Tailwind"],
    "links": {
      "demo": "https://cpg.qmed.ai",
      "repo": null
    },
    "image": "/images/portfolio/AskCPG/page.png",
    "icon": "/images/portfolio/AskCPG/icon.png",
    "gallery": [
      "/images/portfolio/AskCPG/page.png",
      "/images/portfolio/AskCPG/Screenshot 2026-02-11 175923.png"
    ]
  },
  {
    "id": 2,
    "title": "DrMata AI",
    "category": "Medical AI & IoT",
    "role": "Full Stack Engineer",
    "problem": "Diabetic Retinopathy (DR) is a leading cause of preventable blindness, yet rural clinics lack the specialists to diagnose it early. Integrating AI screening with diverse legacy fundus cameras (Topcon, Zeiss, Kowa) posed a significant interoperability challenge.",
    "solution": "Engineered a hardware-agnostic AI integration layer installed on clinic PCs. It interfaces directly with fundus cameras to capture, normalize, and analyze retinal images in real-time, delivering ISO-standard grading (0-2) within 30 seconds.",
    "architecture": "Hybrid Edge-Cloud architecture. Electron-wrapped Vue application manages local hardware handshakes (USB/Serial) and image buffering. AI inference is offloaded to a secure cloud pipeline, with local fallbacks for connectivity drops.",
    "performance": "Achieved 30-second turnaround for full diagnostic reports with 83% clinical accuracy. Optimized image compression reduces bandwidth usage by 65% without compromising diagnostic fidelity.",
    "constraints": ["Interoperability with proprietary camera SDKs (Topcon, Zeiss, Kowa, Drsplus)", "Strict 30-second SLA for results", "Offline-capable for rural deployments"],
    "decisions": [
      "Abstracted vendor-specific camera drivers into a unified 'Device Adapter' pattern, allowing seamless plug-and-play support for new hardware.",
      "Embedded 'AskCPG' guidelines directly into the results view to provide instant, actionable referral pathways."
    ],
    "tradeoffs": [
      "Trade-off: On-device Inference vs Cloud Accuracy. \nDecision: Cloud-based inference for maximum accuracy (83%), with a robust local buffering system (IndexedDB) to handle network instability."
    ],
    "impact": "Partnered with industry leaders (Topcon, Zeiss, Kowa). Enabled early detection of DR for thousands of patients, reducing time-to-diagnosis from 2 weeks to <30 seconds.",
    "stack": ["Electron", "Vue.js", "Python AI", "IndexedDB"],
    "links": {
      "demo": "https://www.drmata.com.my",
      "repo": null
    },
    "image": "/images/portfolio/Drmata/retina viewer.png",
    "icon": "/images/portfolio/Drmata/icon.png",
    "gallery": [
      "/images/portfolio/Drmata/retina viewer.png",
      "/images/portfolio/Drmata/drmata session.png"
    ]
  },
  {
    "id": 3,
    "title": "NoraKPJ Triage System",
    "category": "Enterprise Workflow",
    "role": "Frontend Lead",
    "problem": "High-volume Emergency Departments (ED) suffered from data entry bottlenecks, with triage nurses spending 3+ minutes per patient on redundant form filling.",
    "solution": "Engineered a high-velocity triage interface integrated into the Hospital Information System (HIS). Used state machines to guide nurses through complex clinical decision trees rapidly.",
    "architecture": "Monorepo structure sharing TypeScript interfaces with backend. XState for deterministic finite state machine (FSM) management of triage logic. ",
    "performance": "Sub-100ms interaction latency for all critical paths. Pre-fetching of patient history based on MyKad (ID) scan reduces data entry by 70%.",
    "constraints": ["Legacy integration (SOAP/XML endpoints)", "Zero training time required for nurses", "Must handle 500+ patients/day"],
    "decisions": [
      "Adopted State Machines (XState) over simple boolean logic to prevent 'impossible states' in clinical pathways.",
      "Implemented keyboard-first navigation for rapid data entry without mouse usage."
    ],
    "tradeoffs": [
      "Trade-off: Generic inputs vs Specialized controls. \nDecision: Built custom 'Pain Scale' and 'Body Map' SVG controls for speed, despite higher dev cost than standard dropdowns."
    ],
    "impact": "Cut average triage time by 45% (3m to 1.6m). Reduced queue abandonment rate by 15%. Deployed across 3 KPJ specialist hospitals.",
    "stack": ["Vue.js", "XState", "TypeScript", "SVG Manipulation"],
    "links": {
      "demo": "https://nora.qmed.ai/",
      "repo": null
    },
    "image": "/images/portfolio/norakpj/startpage.png",
    "icon": "/images/portfolio/norakpj/icon.png",
    "gallery": [
      "/images/portfolio/norakpj/startpage.png",
      "/images/portfolio/norakpj/symptom checker.png"
    ]
  },
  {
    "id": 4,
    "title": "QmedScribe",
    "category": "SaaS Platform",
    "role": "Core Developer",
    "problem": "Physicians spend 40% of consultation time typing notes, leading to 'pajama time' (after-hours work) and reduced patient eye contact.",
    "solution": "Built a real-time ambient listening service that transcribes and structures consultation notes using medical-grade ASR (Automatic Speech Recognition).",
    "architecture": "WebAudio API for raw PCM capture. AudioWorklets for custom downsampling (48kHz -> 16kHz) off the main thread. Secure WebSocket streaming to transcription engine.",
    "performance": "Memory stable at <50MB even for hour-long sessions via circular buffer management. Silence detection reduces bandwidth usage by ~50%.",
    "constraints": ["Browser security policies (Autoplay/Microphone)", "Privacy (PII redaction)", "Noisy clinical environments"],
    "decisions": [
      "Moved audio processing to AudioWorklet to prevent UI jank during heavy Reactivity updates.",
      "Implemented client-side VAD (Voice Activity Detection) to stop streaming during silence, saving cloud costs."
    ],
    "tradeoffs": [
      "Trade-off: WebSocket vs HTTP Upload. \nDecision: WebSocket for real-time feedback, providing doctors with 'live confidence' that the system is listening."
    ],
    "impact": "Reclaimed ~2 hours per doctor/day. Used in 50,000+ consultations. 4.8/5 satisfaction rating for 'magical' note generation.",
    "stack": ["WebAudio API", "AudioWorklets", "WebSockets", "Laravel"],
    "deepDive": {
      "diagramType": "flow",
      "nodes": [
        { "id": "mic", "label": "Microphone Input", "type": "input" },
        { "id": "worklet", "label": "AudioWorklet (Processor)", "type": "process" },
        { "id": "vad", "label": "VAD (Silence Detection)", "type": "logic" },
        { "id": "socket", "label": "WebSocket Stream", "type": "network" },
        { "id": "cloud", "label": "ASR Engine (Cloud)", "type": "cloud" },
        { "id": "llm", "label": "LLM Structuring", "type": "ai" },
        { "id": "ui", "label": "Vue Client UI", "type": "output" }
      ],
      "edges": [
        { "from": "mic", "to": "worklet", "label": "PCM Stream (48kHz)" },
        { "from": "worklet", "to": "vad", "label": "Resampled (16kHz)" },
        { "from": "vad", "to": "socket", "label": "Active Speech Chunks" },
        { "from": "socket", "to": "cloud", "label": "Secure Payload" },
        { "from": "cloud", "to": "llm", "label": "Raw Transcript" },
        { "from": "llm", "to": "ui", "label": "Structured JSON" }
      ],
      "description": "The audio pipeline runs entirely off the main thread. We capture raw PCM data, downsample it to 16kHz (optimal for ASR) using a custom AudioWorklet, and apply a Voice Activity Detection (VAD) gate. Only active speech frames are transmitted via WebSocket, reducing bandwidth and cloud costs by half. The returned transcript is then structured by an LLM and pushed back to the client state."
    },
    "links": {
      "demo": "https://scribe.qmed.ai",
      "repo": null
    },
    "image": "/images/portfolio/qmedScribe/home page.png",
    "icon": "/images/portfolio/qmedScribe/icon.png",
    "gallery": [
      "/images/portfolio/qmedScribe/home page.png",
      "/images/portfolio/qmedScribe/audio panel.png",
      "/images/portfolio/qmedScribe/result.png",
      "/images/portfolio/qmedScribe/template.png"
    ]
  }
]
